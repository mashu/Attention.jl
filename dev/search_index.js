var documenterSearchIndex = {"docs":
[{"location":"#Attention.jl","page":"Home","title":"Attention.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Welcome to the documentation for Attention.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package provides modular and extensible attention mechanisms for deep learning models in Julia.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(url=\"https://github.com/mashu/Attention.jl.git\")","category":"page"},{"location":"#Main-Features","page":"Home","title":"Main Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modular attention mechanism interface\nDotProductAttention, NNlibAttention, LinearAttention (currently non-causal and global; causal support planned), MultiHeadAttention\nUtilities like make_causal_mask\nSupport for custom Q/K transformations (e.g., RoPE)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Check out the API Reference for detailed information on the functions and types provided. ","category":"page"},{"location":"api/public/#Public-API","page":"Public API","title":"Public API","text":"","category":"section"},{"location":"api/public/","page":"Public API","title":"Public API","text":"This page lists the public API of Attention.jl.","category":"page"},{"location":"api/public/#Modules","page":"Public API","title":"Modules","text":"","category":"section"},{"location":"api/public/#Attention","page":"Public API","title":"Attention","text":"Attention\n\nA Julia package providing modular and extensible attention mechanisms for deep learning models.\n\nIt offers:\n\nA flexible AbstractAttention interface.\nImplementations like DotProductAttention, NNlibAttention.\nA full MultiHeadAttention layer compatible with Flux.\nUtilities such as make_causal_mask.\nSupport for custom Q/K transformations (e.g., for RoPE in MultiHeadAttention).\n\n\n\n\n\n","category":"module"},{"location":"api/public/#Attention-Mechanisms","page":"Public API","title":"Attention Mechanisms","text":"","category":"section"},{"location":"api/public/#Attention.AbstractAttention","page":"Public API","title":"Attention.AbstractAttention","text":"AbstractAttention\n\nAbstract type for attention mechanisms. Custom implementations should implement the compute_attention method.\n\n\n\n\n\n","category":"type"},{"location":"api/public/#Attention.compute_attention","page":"Public API","title":"Attention.compute_attention","text":"compute_attention(mechanism::AbstractAttention, q, k, v, bias=nothing;\n                mask=nothing, nheads=1, fdrop=identity)\n\nCompute attention based on the specified mechanism.\n\nArguments\n\nmechanism: The attention mechanism to use\nq: Query tensor of shape (dmodel, seqlen_q, batch)\nk: Key tensor of shape (dmodel, seqlen_k, batch)\nv: Value tensor of shape (dmodel, seqlen_v, batch)\nbias: Optional bias tensor\nmask: Optional mask tensor\nnheads: Number of attention heads\nfdrop: Dropout function to apply\n\nReturns\n\noutput: Output tensor of shape (dmodel, seqlen_q, batch)\nattention_weights: Attention weights\n\n\n\n\n\n","category":"function"},{"location":"api/public/#Attention.DotProductAttention","page":"Public API","title":"Attention.DotProductAttention","text":"DotProductAttention <: AbstractAttention\n\nStandard scaled dot-product attention as described in \"Attention is All You Need\" paper.\n\n\n\n\n\n","category":"type"},{"location":"api/public/#Attention.NNlibAttention","page":"Public API","title":"Attention.NNlibAttention","text":"NNlibAttention <: AbstractAttention\n\nAttention implementation that uses NNlib's dotproductattention when available. This provides a more optimized implementation that may be faster in some cases.\n\n\n\n\n\n","category":"type"},{"location":"api/public/#Attention.LinearAttention","page":"Public API","title":"Attention.LinearAttention","text":"LinearAttention <: AbstractAttention\n\nLinear attention implementation that computes attention scores using a feature map φ(x) = elu(x) + 1, following the method described in the paper \"Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\".\n\nThis implementation has linear complexity O(N) with respect to sequence length, compared to quadratic complexity O(N²) of standard dot-product attention.\n\n\n\n\n\n","category":"type"},{"location":"api/public/#Attention.MultiHeadAttention","page":"Public API","title":"Attention.MultiHeadAttention","text":"MultiHeadAttention(d_model, nheads=8; bias=false, dropout_prob=0.0, attention_impl=DotProductAttention(), q_transform=identity, k_transform=identity)\n\nThe multi-head dot-product attention layer used in Transformer architectures.\n\nReturns the transformed input sequence and the attention scores.\n\nArguments\n\nd_model: The embedding dimension\nnheads: number of heads. Default 8.\nbias: whether pointwise QKVO dense transforms use bias. Default false.\ndropout_prob: dropout probability for the attention scores. Default 0.0.\nattention_impl: the attention implementation to use. Default DotProductAttention().\nq_transform: a function to apply to the query tensor after projection. Default identity.\nk_transform: a function to apply to the key tensor after projection. Default identity.\n\n\n\n\n\n","category":"type"},{"location":"api/public/#Utilities","page":"Public API","title":"Utilities","text":"","category":"section"},{"location":"api/public/#Attention.make_causal_mask","page":"Public API","title":"Attention.make_causal_mask","text":"make_causal_mask(x::AbstractArray, dims::Int=2)\n\nCreate a causal mask for a sequence of length derived from x. The mask ensures that position i can only attend to positions j ≤ i.\n\nArguments\n\nx: Input array from which sequence length is derived\ndims: Dimension along which to derive the sequence length (default: 2)\n\nReturns\n\nA boolean mask matrix of shape (seqlen, seqlen) where true indicates allowed attention and false indicates masked (disallowed) attention.\n\n\n\n\n\n","category":"function"},{"location":"api/public/","page":"Public API","title":"Public API","text":"","category":"page"}]
}
